\chapter{Rilevamento delle differenze tra fotogrammi}
\label{cha:diff}

Una funzionalità innovativa introdotta dal sistema LODE è la possibilità per gli studenti di acquisire in tempo reale durante le lezioni degli screenshot di quanto viene proiettato, utilizzando un'apposita applicazione web. Dal punto di vista tecnico questo significa che il "box" deve poter acquisire singolarmente dei fotogrammi dall'ingresso HDMI e inviarli al server.

Per evitare che richieste di screenshot molto vicine provochino inutili upload di screenshot, è possibile implementare un sistema per rilevare se uno screenshot è effettivamente nuovo, e in caso negativo riusare quello precedente.

\section{Il formato YUV420SP}
\label{sec:diff_yuv}

Prima di iniziare a lavorare sul confronto delle immagini, è opportuno familiarizzare con il formato YUV420SP, che si incontra frequentemente nello sviluppo Android con il nome di NV12 o NV21.

Prima di tutto, YUV420 è un termine approssimativo per riferirsi a Y'CbCr 4:2:0 \cite{yuv}, un formato di pixel appartenente alla famiglia Y'CbCr, in cui il colore di un pixel è scomposto nelle componenti Y' (luminanza, scala di grigi\footnotemark{}), Cb/U (proiezione del blu) e Cr/V (proiezione del rosso).

\footnotetext{La suddivisione delle componenti in Y, U e V deriva dal mondo analogico, quando c'era la necessità di supportare sia tv in bianco e nero che a colori. Avere una componente in scala di grigi isolata permette infatti di mantenere la retrocompatibilità pur introducendo il colore. In ambito digitale, è più corretto usare il termine Y'CbCr anziché YUV.}

Questa divisione permette di applicare una tecnica chiamata sottocampionamento della crominanza, che riduce la risoluzione delle due componenti della crominanza (Cb e Cr), lasciando a piena risoluzione la luminanza. Questa tecnica è fondata sul fatto che l'occhio umano è molto più sensibile all'intensità di luce che al colore (figura \ref{fig:diff_mit}), per cui anche se riduciamo i "bit" per la rappresentazione della crominanza la differenza è nella gran parte dei casi quasi impercettibile \cite{luminance}. La grandissima maggioranza dei video normalmente fruibili tramite Internet o la televisione digitale sono codificati con un formato dei pixel Y'CbCr 4:2:0, che è il più diffuso in ambiti non professionali.

% https://twitter.com/AkiyoshiKitaoka/status/1028473566193315841

\begin{figure}[htbp]
	\centering
	
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{res/mit1.png}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{res/mit2.png}
	\end{subfigure}
	
	\caption{Rappresentazione grafica realizzata dal MIT\protect\footnotemark{} per mostrare come l'occhio umano è molto sensibile all'intensità della luce (luminanza). L'illusione ottica fa credere che i due riquadri A e B siano di sfumature di grigio diverse, quando in realtà sono identici.}
	\label{fig:diff_mit}
\end{figure}

\footnotetext{\url{http://persci.mit.edu/gallery/checkershadow}}

La figura \ref{fig:diff_yuv420} mostra il sottocampionamento della crominanza 4:2:0 applicato a una griglia di dimensione 4 x 2 pixel. La componente Y, cioè la luminanza, viene campionata a piena risoluzione, e cioè per ogni pixel vengono catturate informazioni piene. L'informazione sulla crominanza, composta dalla componente blu e rossa\footnotemark{}, viene invece catturata a $1/4$ della risoluzione, e cioè è condivisa tra 4 pixel. In confronto a RGB24, questo formato richiede in media 12 bit per pixel anziché 24 (da qui il nome "NV12"), con un risparmio di dati del 50\% senza sacrificare visibilmente la qualità complessiva.

\footnotetext{L'informazione sul verde non è esplicitamente memorizzata ma rappresenta circa il 60\% della componente Y:\\ \url{https://news.ycombinator.com/item?id=1892248}}

\begin{figure}[htbp]
	\centering
	
	\includegraphics[width=0.8\textwidth]{res/yuv420.pdf}
%	\includegraphics[width=0.9\textwidth]{res/yuv420.png}
	
	\caption{Sottocampionamento della crominanza 4:2:0. Il 4 indica la larghezza della griglia, che ha altezza 2 (fissa). Il 2 indica che la risoluzione orizzontale è dimezzata (2 campioni), mentre lo zero che non ci sono campioni diversi tra la prima e la seconda riga.}
	\label{fig:diff_yuv420}
\end{figure}

Nel momento in cui YUV420 deve essere rappresentato sotto forma di bit, i tre piani (Y, U e V) vengono isolati. Ad esempio, un pixel verrebbe rappresentato così (ogni lettera è un bit) \cite{vlc}:

\definecolor{ChromaBlue}{HTML}{1160c6}
\definecolor{ChromaRed}{HTML}{E60000}
\definecolor{ChromaSP}{HTML}{ff9c00}

\tikzset{
	pixel/.style={node distance=1pt,font=\sffamily},
	pixelY/.style={pixel,fill=black!15},
	pixelU/.style={pixel,fill=ChromaBlue,text=white},
	pixelV/.style={pixel,fill=ChromaRed,text=white},
	pixelSP/.style={pixel,fill=ChromaSP,text=white},
}

\begin{figure}[H]
	\begin{tikzpicture}
	\node[pixelY] (y) {YYYYYYYY};
	\node[pixelU,right=of y] (u) {UU};
	\node[pixelV,right=of u] {VV};
	\end{tikzpicture}
\end{figure}

Mentre in RGB ciascun pixel richiederebbe 8 bit per canale, in YUV420 i due piani della crominanza (blu e rosso) ne richiedono solo $1/4$, cioè 2.

Di conseguenza, 2 pixel sarebbero rappresentati così, dove ciascuna coppia di U/V è relativa a un pixel:

\begin{figure}[H]
	\begin{tikzpicture}
	\node[pixelY] (y) {YYYYYYYY YYYYYYYY};
	\node[pixelU,right=of y] (u) {UU UU};
	\node[pixelV,right=of u] {VV VV};
	\end{tikzpicture}
\end{figure}

Il formato YUV420SP è invece una variante di YUV420 \emph{Semi Planar}, che significa che i piani U e V sono intrecciati in un unico piano. Un pixel YUV420SP (NV12) verrebbe quindi rappresentato così:

\begin{figure}[H]
	\begin{tikzpicture}
	\node[pixelY] (y) {YYYYYYYY};
	\node[pixelSP,right=of y] {UVUV};
	\end{tikzpicture}
\end{figure}

E 2 pixel così:

\begin{figure}[H]
	\begin{tikzpicture}
	\node[pixelY] (y) {YYYYYYYY YYYYYYYY};
	\node[pixelSP,right=of y] {UVUV UVUV};
	\end{tikzpicture}
\end{figure}

Come accennato, esiste anche il formato NV21, supportato da Android, che è uguale a NV12 ma con la differenza che le componenti U e V all'interno del secondo piano sono scambiate, come mostrato nell'esempio:

\begin{figure}[H]
	\begin{tikzpicture}
	\node[pixelY] (y) {YYYYYYYY YYYYYYYY};
	\node[pixelSP,right=of y] {VUVU VUVU};
	\end{tikzpicture}
\end{figure}

Come si nota dagli schemi, in NV12/NV21 le informazioni sulla luminanza sono raggruppate all'inizio e utilizzano 1 byte per pixel. Questa caratteristica tornerà utile nelle prossime sezioni, in combinazione al fatto che Android fornisce le immagini acquisite dall'ingresso HDMI anche in formato NV21.

\section{Confronto pixel per pixel}
\label{sec:diff_full}

La prima possibilità esplorata è la più immediata: catturare i fotogrammi in formato "non compresso" (NV21) e confrontarli byte per byte. La prima coppia di byte che ha valori diversi determina l'esistenza di una differenza tra i due fotogrammi.

Ricordando la classe \texttt{Camera} di Android introdotta nella sezione \ref{sec:hdmi_camera1}, si può sfruttare il metodo \texttt{setPreviewCallback(...)} per ricevere i dati "raw" che vengono mostrati su schermo.\footnote{\url{https://developer.android.com/reference/android/hardware/Camera.PreviewCallback.html}}

\begin{minted}{java}
camera.setPreviewCallback(new PreviewCallback() {
    @Override
    public void onPreviewFrame(byte[] data, Camera camera) {
        // `data` contiene i dati in formato NV21
    }
});
\end{minted}

Supponendo ora di avere due array di byte contenenti due fotogrammi catturati in momenti diversi, si può concludere che se le due immagini sono identiche anche i valori dei pixel e quindi i relativi "byte" saranno identici. Di conseguenza, per confrontare i due array si può usare il metodo \texttt{Arrays.equals(arr1, arr2)} contenuto in \texttt{java.util}, che è implementato con un ciclo che confronta gli array byte per byte e si ferma eventualmente alla prima differenza.\footnote{\url{https://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/Arrays.java\#l2668}}

Questo metodo di rilevamento delle differenze è stato verificato e funziona in modo affidabile, ma ha lo svantaggio non trascurabile di non essere molto performante. Su uno degli hardware testati, il confronto di due fotogrammi con risoluzione 1920 x 1080 (e quindi di $3\,110\,400$ byte\footnote{$(1920 \cdot 1080) / 2$}) richiedeva in media tra i 400 e i 450 millisecondi. Su un altra board simile il tempo medio risultava di circa 100 millisecondi.

Come accennato nella sezione \ref{sec:diff_yuv}, l'occhio umano è più sensibile alla luminanza che alla crominanza. Si può quindi tentare di ridurre la quantità di byte da confrontare selezionando soltanto il piano Y, che rappresenta la luminanza.

In questo caso il confronto va implementato manualmente per considerare soltanto una parte dell'array, quella corrispondente al primo piano, che qua si suppone utilizzi un byte per pixel come nel caso di NV12/NV21:

\begin{minted}{java}
public boolean isLuminanceEqual(byte[] arr1, byte[] arr2, int width, int height) {
    int maxLength = width * height;
    
    for (int i = 0; i < maxLength; i++) {
        if (arr1[i] != arr2[i]) {
            return false;
        }
    }
    
    return true;
}
\end{minted}

Assumendo una risoluzione di 1920 x 1080 pixel, il numero di byte da verificare si riduce a $2\,073\,600$, cioè $2/3$ del totale. La riduzione è interessante ma non risolutiva in termini di prestazioni.

\section{Confronto parziale}
\label{sec:diff_parziale}

Una soluzione alternativa a confrontare l'intera immagine è individuare un sottoinsieme di pixel, distribuito in modo da rendere sufficientemente probabile il rilevamento delle differenze.

Si parte dal presupposto che durante una lezione vengano proiettati contenuti le cui variazioni sono abbastanza evidenti. Si pensi ad esempio a una presentazione di slide dove con il passaggio da una pagina all'altra varia l'intero contenuto proiettato o viene aggiunta una riga di testo, oppure a una finestra del browser dove lo scorrimento della pagina provoca lo spostamento di tutto il contenuto.

Si può quindi costruire una griglia virtuale i cui punti di intersezione rappresentano i pixel da confrontare, come mostrato in figura \ref{fig:diff_slide1}. L'approccio funziona bene in molti casi: prendendo come riferimento la figura si immagini come l'aggiunta di una riga verrebbe facilmente rilevata dai punti stabiliti (che sono fissi).

\tikzset{
	gridline/.style={line width=0.4pt,color=lightgray},
	gridarrow/.style={-{Straight Barb},very thick,red},
}

\begin{figure}[htbp]
	\centering

	\begin{tikzpicture}
	% https://tex.stackexchange.com/questions/9559/drawing-on-an-image-with-tikz

	\node[anchor=south west,inner sep=0] (image) at (0,0)
		{\includegraphics[width=0.9\textwidth]{res/test-slide/beamer.pdf}};
	
	\begin{scope}[x={(image.south east)},y={(image.north west)}]
%	\draw[color=black,thick,xstep=.1,ystep=.1] (0,0) grid (1,1);
	\foreach \x in {1,...,9} { \draw[gridline] (\x/10,0) -- (\x/10,1); }
	\foreach \y in {1,...,49} { \draw[gridline] (0,\y/50) -- (1,\y/50); }
	\end{scope}
	
	\end{tikzpicture}
	
	\caption{La griglia è composta da 9 righe orizzontali e 49 verticali, per un totale di 441 punti di intersezione.}
	\label{fig:diff_slide1}
\end{figure}

Ci sono tuttavia altri casi in cui i cambiamenti potrebbero riguardare delle aree non rilevate dai punti selezionati, come si deduce dalla figura \ref{fig:diff_slide2}.

\begin{figure}[htbp]
	\centering
	
	\begin{subfigure}[t]{0.5\textwidth}
		\begin{tikzpicture}
		\node[anchor=south west,inner sep=0] (image) at (0,0)
		{\includegraphics[width=\textwidth]{res/slide3.png}};
		
		\begin{scope}[x={(image.south east)},y={(image.north west)}]
		\foreach \x in {1,...,9} { \draw[gridline] (\x/10,0) -- (\x/10,1); }
		\foreach \y in {1,...,49} { \draw[gridline] (0,\y/50) -- (1,\y/50); }
		
		\draw[gridarrow] (0.62,0.15) -- (0.68,0.29);
		\end{scope}
		
		\end{tikzpicture}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\begin{tikzpicture}
		\node[anchor=south west,inner sep=0] (image) at (0,0)
		{\includegraphics[width=\textwidth]{res/slide4.png}};
		
		\begin{scope}[x={(image.south east)},y={(image.north west)}]
		\foreach \x in {1,...,9} { \draw[gridline] (\x/10,0) -- (\x/10,1); }
		\foreach \y in {1,...,49} { \draw[gridline] (0,\y/50) -- (1,\y/50); }
		
		\draw[gridarrow] (0.62,0.15) -- (0.68,0.29);
		\end{scope}
		
		\end{tikzpicture}
	\end{subfigure}
	
	\caption{Anche in questo caso i punti di intersezione sono 441. Si noti però che l'area puntata dalla freccia presenta delle differenze tra i due fotogrammi che non vengono rilevate. Le immagini sono state gentilmente offerte dal professor Alberto Montresor.}
	\label{fig:diff_slide2}
\end{figure}

Si possono quindi considerare delle strategie alternative che permettono di aggirare questo problema. Ad esempio:

\begin{itemize}
	\item il numero di colonne può essere aumentato in modo da rendere la griglia più "fitta" e rilevare non solo aggiunte di frasi ma anche brevi parole o simboli;
	\item il numero di righe e colonne della griglia può essere ricalcolato ad ogni controllo aggiungendo un fattore di casualità. Se un controllo non riesce a rilevare differenze, la volta successiva potrebbe invece riuscirci, dato che i pixel campione saranno diversi (con ragionevole probabilità);
	\item la distanza orizzontale e verticale tra i punti può essere mantenuta costante, ma si può invece aggiungere uno scorrimento graduale verso destra e verso il basso dei punti, in modo da coprire aree diverse ad ogni controllo;
	\item la strategia di scegliere un numero casuale di righe e colonne può essere resa più intelligente, per evitare che la stessa coordinata orizzontale e/o verticale venga considerata in diverse combinazioni della griglia.
\end{itemize}

La strategia del secondo punto è stata sperimentata su diversi contenuti e ha dato ottimi risultati. Gran parte dei cambiamenti venivano rilevati al primo controllo, e in ogni caso entro 3-4 controlli. Nei test l'intervallo di verifica delle differenze era 500 millisecondi, per cui il ritardo massimo di rilevamento risultava di circa 2 secondi.

Nei test effettuati il numero di colonne e righe della griglia veniva scelto in modo casuale ad ogni controllo, scegliendo rispettivamente dall'intervallo $[10, 20]$ e $[60, 80]$. Con questi dati, nel peggiore dei casi il numero di pixel/byte da verificare è $1\,600$, un valore estremamente limitato se confrontato con i 3 milioni di byte della sezione \ref{sec:diff_full}. Le prestazioni di questa soluzione sono risultate di conseguenza ottime, con un tempo di confronto solitamente non superiore a 1-2 millisecondi.

\section{Estrazione slide in post-elaborazione}
\label{sec:diff_postprod}

Un'altra possibile applicazione del rilevamento delle differenze è l'estrazione di "istantanee" dal video registrato, in fase di post-elaborazione. I fotogrammi estratti potrebbe poi essere utilizzati come indici legati a punti specifici del video, consentendo di costruire funzionalità come il salto preciso a una slide, oppure in generale come anteprime rappresentative del video.

Per estrarre tutti i fotogrammi "unici" da un video ci viene ancora una volta in aiuto \texttt{ffmpeg}, in particolare con un filtro chiamato \texttt{freezedetect} introdotto nella versione 4.2 (agosto 2019).

Il filtro \texttt{freezedetect} prevede due opzioni \cite{ffmpeg}:

\begin{itemize}
	\item \texttt{noise}: indica la soglia di rumore sopra la quale due fotogrammi vengono considerati diversi. Questa soglia può essere specificata in decibel (aggiungendo dB al valore), oppure con un numero decimale nell'intervallo $[0, 1]$. Il valore predefinito è $-60 dB$, equivalente a $0,001$;
	\item \texttt{duration}: l'intervallo di tempo oltre il quale un fotogramma viene considerato in stato di "freeze", e quindi invariato secondo il parametro \texttt{noise}. Il valore predefinito è 2 secondi.
\end{itemize}

Il comando che segue configura il filtro \texttt{freezedetect} con una soglia di rumore pari a $0,01$ e una durata minima di 5 secondi. Questi parametri sono stati ricavati empiricamente utilizzando come input dei video catturati durante reali lezioni universitarie. L'intervallo di 5 secondi è in particolare pensato per evitare che cambi momentanei della schermata vengano considerati rilevanti.

\begin{minted}{bash}
> ffmpeg -i input.mp4 \
         -vf "freezedetect=noise=0.01:duration=5,metadata=print:file=log.txt" \
         -an -f null -
\end{minted}

Nel comando si può notare che oltre a \texttt{freezedetect} viene applicato un secondo filtro chiamato \texttt{metadata}. Questo filtro si occupa di leggere i valori prodotti dal filtro \texttt{freezedetect} e di scriverli in un file chiamato \texttt{log.txt}.

I metadati prodotti sono identificati da tre chiavi, in particolare:

\begin{itemize}
	\item \texttt{lavfi.freezedetect.freeze\_start}: indica il \emph{timestamp} del primo fotogramma in cui è stato individuato il freeze (inclusi i primi secondi di \texttt{duration});
	\item \texttt{lavfi.freezedetect.freeze\_duration}: indica in secondi l'intervallo totale durante il quale il fotogramma è rimasto invariato;
	\item \texttt{lavfi.freezedetect.freeze\_end}: indica il \emph{timestamp} dell'ultimo fotogramma dell'intervallo.
\end{itemize}

Dopo aver eseguito il comando, il file \texttt{log.txt} conterrà i dati raccolti. Nell'esempio che segue sono state rilevate 3 istantanee, ai secondi 0, 132 e 200.

\begin{figure}[H]
\begin{minted}[highlightlines={2,7,12}]{text}
frame:192  pts:451200  pts_time:5.01333
lavfi.freezedetect.freeze_start=0
frame:5416 pts:11594764 pts_time:128.831
lavfi.freezedetect.freeze_duration=128.831
lavfi.freezedetect.freeze_end=128.831
frame:5764 pts:12334264 pts_time:137.047
lavfi.freezedetect.freeze_start=132.031
frame:8439 pts:18041731 pts_time:200.464
lavfi.freezedetect.freeze_duration=68.433
lavfi.freezedetect.freeze_end=200.464
frame:8648 pts:18493226 pts_time:205.48
lavfi.freezedetect.freeze_start=200.464
\end{minted}
\caption{Contenuto del file di output \texttt{log.txt}.}
\label{fig:diff_freezeout}
\end{figure}

Se si vogliono ora estrarre i fotogrammi individuati, ci sono due strade. La prima è quella di ritagliare più volte i primi N secondi del video (con \texttt{-ss N}) e poi estrarre il primo fotogramma immediatamente successivo (\texttt{-frames:v 1}):

\begin{minted}{bash}
> ffmpeg -ss 1 -i input.mp4 \
         -ss 133 -i input.mp4 \
         -ss 201 -i input.mp4 \
         -map 0:v -frames:v 1 out1.png \
         -map 1:v -frames:v 1 out2.png \
         -map 2:v -frames:v 1 out3.png -y
\end{minted}

Si osservi che l'offset in secondi è stato incrementato di uno e arrotondato per difetto, in modo da evitare eventuali artefatti di codifica dovuti al cambio di scena.

La soluzione precedente ha lo svantaggio di caricare più volte il file in parallelo, causando un uso di memoria RAM molto elevato nel caso in cui i fotogrammi da estrarre siano tanti (più di qualche decina).

L'alternativa è quindi usare il filtro \texttt{select}, che permette di selezionare solo determinati fotogrammi dall'input e di passarli all'output. Questo metodo fa un uso molto limitato di memoria ma è anche molto più lento, perché non sfrutta la funzionalità di seeking rapido offerta dall'opzione \texttt{-ss}.

Sono inoltre richiesti gli indici dei fotogrammi, anziché i relativi \emph{timestamp}, che vanno quindi estratti dall'output mostrato nella figura \ref{fig:diff_freezeout}. In compenso non è però necessario aggiungere un offset ai valori, perché gli indici dei fotogrammi sono già relativi a un momento successivo di 5 secondi rispetto all'inizio del freeze.

\begin{minted}{bash}
> ffmpeg -i input.mp4 \
         -vf "select='eq(n,192)+eq(n,5764)+eq(n,8648)'" \
         -vsync 0 %d.png
\end{minted}

In questo caso c'è da notare l'aggiunta dell'opzione \texttt{-vsync 0}, che ha lo scopo di disabilitare la duplicazione dei fotogrammi che viene normalmente applicata per simulare un framerate costante.

A questo punto un possibile uso dei file generati è la generazione di un mosaico, con la possibilità di configurare la dimensione delle singole immagini e il numero di colonne e righe della griglia:

\begin{minted}{bash}
> ffmpeg -i %d.png -vf "scale=200:-1,tile=5x3" \
         -frames:v 1 tiles.png -y
\end{minted}

